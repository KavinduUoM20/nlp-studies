{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "909cf128-9ea8-486a-8401-2bc3438b7996",
   "metadata": {},
   "source": [
    "## Tokenization using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b306af6f-0538-44ad-a6ed-c20f52dc6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60410a24-d1a0-4675-a8b2-dce4addd46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create language object\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Dr. Smith, renowned for his groundbreaking research in neuroscience, will deliver the keynote address at the conference. After years of dedicated study, Sarah finally achieved her dream of becoming Dr. Johnson, earning her Ph.D. in astrophysics at a cost of $2000.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4288343f-10ef-4e20-ae2e-e4048c7d3524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Smith\n",
      ",\n",
      "renowned\n",
      "for\n",
      "his\n",
      "groundbreaking\n",
      "research\n",
      "in\n",
      "neuroscience\n",
      ",\n",
      "will\n",
      "deliver\n",
      "the\n",
      "keynote\n",
      "address\n",
      "at\n",
      "the\n",
      "conference\n",
      ".\n",
      "After\n",
      "years\n",
      "of\n",
      "dedicated\n",
      "study\n",
      ",\n",
      "Sarah\n",
      "finally\n",
      "achieved\n",
      "her\n",
      "dream\n",
      "of\n",
      "becoming\n",
      "Dr.\n",
      "Johnson\n",
      ",\n",
      "earning\n",
      "her\n",
      "Ph.D.\n",
      "in\n",
      "astrophysics\n",
      "at\n",
      "a\n",
      "cost\n",
      "of\n",
      "$\n",
      "2000\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#By default it makes word tokens\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af5220ee-cfa2-4e50-9316-24175e0047cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cefb1c3-8d86-4a5e-85cb-64806c312711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f937ce4f-5a52-4f5a-8cba-4da7a824d9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9135a42a-14c3-4aca-9419-64fc7e0667fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "919008c7-46a2-45eb-b4b3-9b14c223d333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a2be0ce-be56-41f7-ad3d-8d12701f8394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token45 = doc[45]\n",
    "token45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e77a4fa-535d-46a6-978b-f60797cf2d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token45.is_currency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb113c69-f922-49ac-ad50-13c2302d187a",
   "metadata": {},
   "source": [
    "#### Extract Emails Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3bdcaf2-07f7-49d1-88fd-e97a334dc4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./public/students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68f5f59d-c3bd-4b25-989d-938c11c8c81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cac6bf94-782d-41df-8d4f-cfee4ffc05c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[virat@kohli.com, maria@sharapova.com, serena@williams.com, joe@root.com]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token)\n",
    "\n",
    "emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8d281-4901-4dbb-a398-edfec7881559",
   "metadata": {},
   "source": [
    "#### Customize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28004ddf-be5c-4291-bfaf-741a313e11a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'spicy', 'pizza']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(\"gimme double cheese extra large spicy pizza\")\n",
    "\n",
    "tokens = [token.text for token in doc2]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b36927b3-71d1-4648-aaf9-4d781312706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\",[{ORTH:\"gim\"},{ORTH:\"me\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ff5acac-f69c-480d-9e66-4d54ed0bbba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'spicy', 'pizza']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(\"gimme double cheese extra large spicy pizza\")\n",
    "\n",
    "tokens = [token.text for token in doc2]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769cda1-e671-4ed5-b057-5d12a2f6468b",
   "metadata": {},
   "source": [
    "We can't change the actual text, but can split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1d2fa-07c1-48ff-9cf2-2e2ad6fbf929",
   "metadata": {},
   "source": [
    "#### Add Components to Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c560fb11-99c6-4979-86de-637cb03295fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m doc3 \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDr. Smith, renowned for his groundbreaking research in neuroscience, will deliver the keynote address at the conference. After years of dedicated study, Sarah finally achieved her dream.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msents\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\PROJECTS - AI\\nlp-studies\\env\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:923\u001b[0m, in \u001b[0;36msents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"Dr. Smith, renowned for his groundbreaking research in neuroscience, will deliver the keynote address at the conference. After years of dedicated study, Sarah finally achieved her dream.\")\n",
    "\n",
    "for sentence in doc3.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ef18bc8-468d-4ae0-9065-c9ef9975637d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1cb7b566110>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6d07750-3046-40ce-aebb-1b2c4a63fd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84ba0539-ea32-4986-983f-889dadf21ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Smith, renowned for his groundbreaking research in neuroscience, will deliver the keynote address at the conference.\n",
      "After years of dedicated study, Sarah finally achieved her dream.\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"Dr. Smith, renowned for his groundbreaking research in neuroscience, will deliver the keynote address at the conference. After years of dedicated study, Sarah finally achieved her dream.\")\n",
    "\n",
    "for sentence in doc3.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972bfe34-a819-4d96-b503-fa45b9365304",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4d91128-0931-432d-b203-e80a35c6849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e51505e-a971-4fc5-9ff8-10a3ae8fac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be3f1e22-dc3b-4347-8a9c-99841443a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [token.text for token in para if token.like_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7065ebfd-9aeb-4b7b-af27-fb0e1cde861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0673487-484c-4d9a-9db3-5e981e88ad37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "tokens = nlp(transactions)\n",
    "for token in tokens:\n",
    "    if token.like_num and tokens[token.i+1].is_currency:\n",
    "        print(token,tokens[token.i+1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae04a8-9c9b-4bf6-850d-84381010525f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b386abed-5ba8-4dc9-90a4-1ca08d5f5cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
